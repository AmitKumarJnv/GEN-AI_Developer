{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8565d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting weaviate-client\n",
      "  Downloading weaviate_client-4.19.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: httpx<0.29.0,>=0.26.0 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from weaviate-client) (0.27.0)\n",
      "Collecting validators<1.0.0,>=0.34.0 (from weaviate-client)\n",
      "  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting authlib<2.0.0,>=1.6.5 (from weaviate-client)\n",
      "  Downloading authlib-1.6.6-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting pydantic<3.0.0,>=2.12.0 (from weaviate-client)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting grpcio<1.80.0,>=1.59.5 (from weaviate-client)\n",
      "  Downloading grpcio-1.76.0-cp310-cp310-macosx_11_0_universal2.whl.metadata (3.7 kB)\n",
      "Collecting protobuf<7.0.0,>=4.21.6 (from weaviate-client)\n",
      "  Downloading protobuf-6.33.2-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting deprecation<3.0.0,>=2.1.0 (from weaviate-client)\n",
      "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: cryptography in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from authlib<2.0.0,>=1.6.5->weaviate-client) (44.0.1)\n",
      "Requirement already satisfied: packaging in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from deprecation<3.0.0,>=2.1.0->weaviate-client) (24.2)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from grpcio<1.80.0,>=1.59.5->weaviate-client) (4.12.1)\n",
      "Requirement already satisfied: anyio in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (4.4.0)\n",
      "Requirement already satisfied: certifi in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (3.7)\n",
      "Requirement already satisfied: sniffio in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.29.0,>=0.26.0->weaviate-client) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.12.0->weaviate-client) (0.7.0)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.12.0->weaviate-client)\n",
      "  Downloading pydantic_core-2.41.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting typing-extensions~=4.12 (from grpcio<1.80.0,>=1.59.5->weaviate-client)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.12.0->weaviate-client)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from anyio->httpx<0.29.0,>=0.26.0->weaviate-client) (1.2.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from cryptography->authlib<2.0.0,>=1.6.5->weaviate-client) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from cffi>=1.12->cryptography->authlib<2.0.0,>=1.6.5->weaviate-client) (2.22)\n",
      "Downloading weaviate_client-4.19.2-py3-none-any.whl (603 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m603.7/603.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading authlib-1.6.6-py2.py3-none-any.whl (244 kB)\n",
      "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading grpcio-1.76.0-cp310-cp310-macosx_11_0_universal2.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.33.2-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp310-cp310-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading validators-0.35.0-py3-none-any.whl (44 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: validators, typing-extensions, protobuf, deprecation, typing-inspection, pydantic-core, grpcio, pydantic, authlib, weaviate-client\n",
      "\u001b[2K  Attempting uninstall: typing-extensions\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.12.1\n",
      "\u001b[2K    Uninstalling typing_extensions-4.12.1:\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.12.1━━━━━━━━━━━\u001b[0m \u001b[32m 1/10\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: pydantic-core━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/10\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: pydantic_core 2.18.3━━━━━━━━━\u001b[0m \u001b[32m 1/10\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling pydantic_core-2.18.3:━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/10\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled pydantic_core-2.18.3━━━━━━━━━━━\u001b[0m \u001b[32m 1/10\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: pydantic\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/10\u001b[0m [grpcio]nsions]\n",
      "\u001b[2K    Found existing installation: pydantic 2.7.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/10\u001b[0m [grpcio]\n",
      "\u001b[2K    Uninstalling pydantic-2.7.2:\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/10\u001b[0m [grpcio]\n",
      "\u001b[2K      Successfully uninstalled pydantic-2.7.20m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/10\u001b[0m [grpcio]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/10\u001b[0m [weaviate-client] [weaviate-client]\n",
      "\u001b[1A\u001b[2KSuccessfully installed authlib-1.6.6 deprecation-2.1.0 grpcio-1.76.0 protobuf-6.33.2 pydantic-2.12.5 pydantic-core-2.41.5 typing-extensions-4.15.0 typing-inspection-0.4.2 validators-0.35.0 weaviate-client-4.19.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting langchain\n",
      "  Downloading langchain-1.2.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.2.1 (from langchain)\n",
      "  Downloading langchain_core-1.2.6-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting langgraph<1.1.0,>=1.0.2 (from langchain)\n",
      "  Downloading langgraph-1.0.5-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from langchain) (2.12.5)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.2.1->langchain)\n",
      "  Downloading langsmith-0.6.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (4.15.0)\n",
      "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core<2.0.0,>=1.2.1->langchain)\n",
      "  Downloading uuid_utils-0.12.0-cp39-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting langgraph-sdk<0.4.0,>=0.3.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading langgraph_sdk-0.3.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading xxhash-3.6.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading ormsgpack-1.12.1-cp310-cp310-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (2.32.3)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain)\n",
      "  Downloading zstandard-0.25.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: anyio in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (4.4.0)\n",
      "Requirement already satisfied: certifi in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.7)\n",
      "Requirement already satisfied: sniffio in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (2.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.2.1)\n",
      "Downloading langchain-1.2.1-py3-none-any.whl (105 kB)\n",
      "Downloading langchain_core-1.2.6-py3-none-any.whl (489 kB)\n",
      "Downloading langgraph-1.0.5-py3-none-any.whl (157 kB)\n",
      "Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
      "Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
      "Downloading langgraph_sdk-0.3.1-py3-none-any.whl (66 kB)\n",
      "Downloading langsmith-0.6.1-py3-none-any.whl (282 kB)\n",
      "Downloading uuid_utils-0.12.0-cp39-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (603 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m603.2/603.2 kB\u001b[0m \u001b[31m820.4 kB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading ormsgpack-1.12.1-cp310-cp310-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (376 kB)\n",
      "Downloading xxhash-3.6.0-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading zstandard-0.25.0-cp310-cp310-macosx_11_0_arm64.whl (640 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m640.6/640.6 kB\u001b[0m \u001b[31m852.5 kB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm-:--:--\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: zstandard, xxhash, uuid-utils, ormsgpack, langsmith, langgraph-sdk, langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph, langchain\n",
      "\u001b[2K  Attempting uninstall: langsmith\n",
      "\u001b[2K    Found existing installation: langsmith 0.1.147\n",
      "\u001b[2K    Uninstalling langsmith-0.1.147:\n",
      "\u001b[2K      Successfully uninstalled langsmith-0.1.147\n",
      "\u001b[2K  Attempting uninstall: langchain-core\n",
      "\u001b[2K    Found existing installation: langchain-core 0.2.43\n",
      "\u001b[2K    Uninstalling langchain-core-0.2.43:\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/11\u001b[0m [langchain-core]\n",
      "\u001b[2K      Successfully uninstalled langchain-core-0.2.43━━━━━━━━━━\u001b[0m \u001b[32m 6/11\u001b[0m [langchain-core]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/11\u001b[0m [langchain]11\u001b[0m [langchain]core]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-openai 0.1.10 requires langchain-core<0.3,>=0.2.2, but you have langchain-core 1.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-1.2.1 langchain-core-1.2.6 langgraph-1.0.5 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.3.1 langsmith-0.6.1 ormsgpack-1.12.1 uuid-utils-0.12.0 xxhash-3.6.0 zstandard-0.25.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: openai in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (1.30.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
      "Requirement already satisfied: certifi in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install weaviate-client\n",
    "!pip install langchain\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965a82d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da4bfea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "# Best practice: store your credentials in environment variables\n",
    "weaviate_url = os.environ[\"WEAVIATE_URL\"]\n",
    "weaviate_api_key = os.environ[\"WEAVIATE_API_KEY\"]\n",
    "\n",
    "# Connect to Weaviate Cloud\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=weaviate_url,\n",
    "    auth_credentials=Auth.api_key(weaviate_api_key),\n",
    ")\n",
    "\n",
    "print(client.is_ready())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d9d9569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-community pypdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a348733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amitkumarsingh/Desktop/GEN-AI_Developer/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"./source_data/YOLO.pdf\"\n",
    "loader = PyPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2ccdfe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 0, 'page_label': '1'}, page_content='Recitation 13\\nSamruddhi Pai, Yashash Gaurav, Talha Faiz\\nSome of the content is inspired by CMU’s 16-824: Visual Learning and Recognition course')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4abb987b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'PyPDF',\n",
      " 'creator': 'Google',\n",
      " 'creationdate': '',\n",
      " 'title': 'YOLO - Recitation',\n",
      " 'source': './source_data/YOLO.pdf',\n",
      " 'total_pages': 83,\n",
      " 'page': 0,\n",
      " 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pp(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3fae825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Recitation 13\\n'\n",
      " 'Samruddhi Pai, Yashash Gaurav, Talha Faiz\\n'\n",
      " 'Some of the content is inspired by CMU’s 16-824: Visual Learning and '\n",
      " 'Recognition course')\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd2be17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 0, 'page_label': '1'}, page_content='Recitation 13\\nSamruddhi Pai, Yashash Gaurav, Talha Faiz\\nSome of the content is inspired by CMU’s 16-824: Visual Learning and Recognition course'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 1, 'page_label': '2'}, page_content='Agenda\\n1. Introduction to Image Segmentation.\\na. Problem definition\\nb. Standard Datasets\\n2. Why YOLO\\n3. Various versions of YOLO\\n4. Image segmentation - SOTA Architectures'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 2, 'page_label': '3'}, page_content='1. Introduction to Image Processing.\\na. Problem definitions - \\ni. Object detection\\nii. Image Segmentation\\nb. Standard Datasets\\n2. R-CNN Family \\n3. YOLO Family\\na. Various versions of YOLO (v1 - v7)\\n4. Image Processing - SOTA Architectures\\nKey Idea - Understanding the journey behind refining models for better speed \\nand/or accuracy for a particular task\\nAgenda'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 3, 'page_label': '4'}, page_content='Problem Definition\\n[Img Ref: https://www.researchgate.net/figure/Comparison-of-semantic-segmentation-classification-and-localization-object-detection_fig1_334363440]'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 4, 'page_label': '5'}, page_content='Object Detection\\n● Classifying multiple instances of objects and localizing within an image\\n● One-Stage Methods - \\nBetter inference speed - \\nYOLO, SSD and RetinaNet\\n● Two-Stage Methods - \\nBetter Accuracy - Faster \\nR-CNN, Mask   R-CNN and \\nCascade R-CNN\\n[Img Ref: https://www.researchgate.net/figure/Two-stage-vs-one-stage-object-detection-models_fig3_353284602]'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 5, 'page_label': '6'}, page_content='Image Segmentation\\n[Img Ref: https://www.researchgate.net/figure/b-semantic-segmentation-c-instance-segmentation-and-d-panoptic-segmentation-for_fig5_342409316]'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 6, 'page_label': '7'}, page_content='Datasets\\n● ImageNet - Large dataset containing annotated images based on WordNet’s \\nhierarchical structure\\n● PASCAL VOC - Pattern Analysis, Statistical Modelling and Computational \\nLearning - Visual Object Classes Dataset\\n● MS COCO - Microsoft Common Objects in Context\\n● STL-10 : Subset of ImageNet with 10 Classes. Also has unlabeled images\\n● CIFAR-x : x defines the number of classes\\nAll of these can be used for object detection, segmentation, dense pose \\nestimation, key point detection, etc.\\nOther task specific datasets - MNIST, KITTI, etc.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 7, 'page_label': '8'}, page_content='Different Methods for Solving these tasks:\\n● Edge templates + nearest neighbor\\n● Haar Wavelets + SVM\\n● Other wavelet + adaBoost etc.\\n● Rectangular differential features + adaBoost\\n● Parts based binary orientation position histograms + adaBoost\\n● Dynamic programming\\n● Learning based methods'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 8, 'page_label': '9'}, page_content='Learning Based Methods\\n[Ref: https://www.mdpi.com/2079-9292/9/4/583]'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 9, 'page_label': '10'}, page_content='RCNN - Region-based Convolutional Neural Network [2]\\n● 2K regions are proposed \\nand reshapes\\n● These are passed through \\nCNN architecture for \\nextracting features (AlexNet, \\nVGG 16, ResNet 50)\\n● Classification and \\nlocalization is performed by \\nSVMs and fully connected \\nlayers respectively\\n[Ref: https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e]'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 10, 'page_label': '11'}, page_content='Proposing the Regions'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 11, 'page_label': '12'}, page_content='Problems with RCNN\\n● Multistage training is expensive and time consuming - Pretrained CNN \\nnetwork is fine-tune on proposed regions and then final layer is replaced by \\nSVM classifier\\n● Objects are detected over each of the regions which is time consuming. \\nPositive and negative samples based on IOU are generated after this stage'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 12, 'page_label': '13'}, page_content='Fast RCNN [3]\\n● Inspired by SPPNet, Fast RCNN \\nuses ROI pooling where CNN is \\napplied only once over the resized \\nimage and features corresponding to \\nthe proposed regions are selected\\n● Reduced processing time as \\nfeatures are not extracted for each \\nselected region\\n● Since the image is resized, a fixed \\nsize vector is obtained at the output \\nwhich is passed to fully connected \\nlayers for predicting classes and \\nbounding box parameters'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 13, 'page_label': '14'}, page_content='Faster RCNN [4]\\n● Selective search algorithm is \\nreplaced by learning based \\nmethod for proposing regions → \\nfaster\\n● These regions are then reshaped \\nusing ROI pooling to extract \\nfeatures\\n● Fully connected layers predict \\nthe objects and bounding boxed \\nfor them\\n● Mask RCNN [5] uses the same \\narchitecture but has addition \\nhead to generate pixel-by-pixel \\nbinary mask for each class'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 14, 'page_label': '15'}, page_content='Region Proposal Network [4]\\n● Proposes k boxes for each \\nsliding window\\n● Classification layer for \\nobjectness score (0 or 1)\\n● Regression layer to \\npredict 4 coordinates for \\neach box'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 15, 'page_label': '16'}, page_content='Mask R-CNN[5]\\n● Extends Faster R-CNN by adding a \\nbranch for predicting an object mask \\nin parallel with the existing branch for \\nbounding box recognition\\n● Is easy to generalize to other tasks, \\ne.g., allowing us to estimate human \\nposes in the same framework - using \\nmultiple Feature Pyramid Network \\n(FPN)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 16, 'page_label': '17'}, page_content='ROIAlign ● ROIPool - Quantize floating \\npoint values → Subdivide into \\nspatial bins → aggregate the \\nvalues\\n● Quantization error was \\nacceptable for bounding \\nboxes but but cannot be used \\nfor generating sharp masks\\n● ROIAlign - Use binary \\ninterpolations based on 4 \\ncorners → aggregate the \\nfeatures'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 17, 'page_label': '18'}, page_content='Results on COCO dataset [3]'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 18, 'page_label': '19'}, page_content='Results on COCO dataset [5]'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 19, 'page_label': '20'}, page_content='Mask R-CNN\\n● Proposed by Kaiming He et.al. in \\n2017 \\n(https://arxiv.org/pdf/1703.06870.pdf)\\n● Extends Faster R-CNN by adding a \\nbranch for predicting an object mask \\nin parallel with the existing branch for \\nbounding box recognition\\n● Is easy to generalize to other tasks, \\ne.g., allowing us to estimate human \\nposes in the same framework'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 20, 'page_label': '21'}, page_content='Mask R-CNN'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 21, 'page_label': '22'}, page_content=''),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 22, 'page_label': '23'}, page_content='YOLO - You Only Look Once\\n- Belong to a new family of Object Detection Networks: Single Shot Detectors\\n- Takes a single shot of the image to detect multiple objects, \\n- R-CNN Series have a separate Region Proposal Network (RPN) and then a network for \\ndetecting objects from each proposal \\n- Much faster than R-CNN models - Faster R-CNN (73.2% mAP at 7 FPS) and YOLOv1 \\n(63.4% mAP at 45 FPS)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 23, 'page_label': '24'}, page_content='[img ref: https://pyimagesearch.com/2022/04/04/introduction-to-the-yolo-family/]\\nThe YOLO Family'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 24, 'page_label': '25'}, page_content='YOLOv1\\nRefs:\\n- https://arxiv.org/pdf/1506.02640.pdf\\n- https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89\\nMotives and Concepts:\\n- Divides the input image into an S × S (7x7) grid.\\n- If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object.\\n- Each grid cell predicts B (2) bounding boxes and confidence scores for those boxes. These confidence scores reflect how \\nconfident the model is that the box contains an object and also how accurate it thinks the box is that it predicts.\\n- Formally we define confidence as Pr(Object) ∗ IOU. If no object exists in that cell, the confidence score should be zero. \\nOtherwise, we want the confidence score to equal the intersection over union (IOU) between the predicted box and the ground \\ntruth.\\n- \\n[Img ref: https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/]'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 25, 'page_label': '26'}, page_content='YOLOv1\\nIllustration\\n-'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 26, 'page_label': '27'}, page_content='YOLOv1\\nArchitecture:\\n- Inspired by GoogLeNet\\n- Alternating 1 × 1 convolutional layers reduce the features space from preceding layers\\n-'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 27, 'page_label': '28'}, page_content='YOLOv1\\nOutput and how it makes sense:\\nImg ref: https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 28, 'page_label': '29'}, page_content='YOLOv1\\nLoss:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 29, 'page_label': '30'}, page_content='YOLOv1\\nPerformance\\nOn PASCAL VOC:\\nPattern Analysis, Statistical Modelling and Computational Learning - Visual Object Classes Dataset'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 30, 'page_label': '31'}, page_content='YOLOv2 or YOLO9000: Better, Faster, Stronger\\nRefs:\\n- https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-o\\nbject-detection-7883d2b02a65 \\n- https://arxiv.org/pdf/1612.08242.pdf \\nPerformance:\\n- At 67 FPS, YOLOv2 gets 76.8% mAP on PASCAL VOC 2007. \\n- At 40 FPS, YOLOv2 gets 78.6% mAP which is better than Faster R-CNN \\nusing ResNet and SSD.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 31, 'page_label': '32'}, page_content='Improvements on YOLOv1:\\n- Batch Normalization - 2% improvement\\n- Double input image resolution - 4% mAP improvement\\n- No Fully connected layers\\n- Conv only architecture in which each Anchor Box predicts bounding boxes in its region\\n- Class and objectness is calculated for each anchor box - +7% recall\\n- Calculated the size of anchor boxes using K (5) means clustering\\n- Trained on variety of different input image dimensions (320*320 - 608*608)\\n- Trained for classification (MS COCO dataset) and object detection (stronger)\\n- Combining classes from MS COCO and ImageNet by hierarchically clustering classes - finally 9418 classes - \\n19.7% mAP \\nYOLOv2 or YOLO9000: Better, Faster, Stronger'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 32, 'page_label': '33'}, page_content='YOLOv2 or YOLO9000: Better, Faster, Stronger\\nHow to formulate Anchor Boxes: For every bounding box, we would predict the tx, ty, tw, th, and to which \\ninform the following:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 33, 'page_label': '34'}, page_content='YOLOv2 or YOLO9000: Better, Faster, Stronger\\n ● Ablation results:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 34, 'page_label': '35'}, page_content='YOLOv3: An Incremental Improvement\\nRefs:\\n- https://arxiv.org/pdf/1804.02767.pdf - Fun Read :D\\n- https://towardsdatascience.com/review-yolov3-you-only-look-once-object-dete\\nction-eab75d7a1ba6 \\nSome high level changes:\\n- Class predictions - Softmax is not used, independent logistic classifiers are \\nused with binary cross entropy loss (classes are not mutually exclusive - eg. \\ndifferent datasets)\\n- 9 anchor boxes with 3 of each scale - you should calculate your own \\n(K-Means)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 35, 'page_label': '36'}, page_content='YOLOv3:An Incremental Improvement\\nArchitecture:\\n- DarkNet 53 (from DN19) - better feature extractor with residual connections\\n- Feature map upsampling and residual connection - access to finer details at \\nmultiple level - Detection at 3 different levels (down-sampled by 32, 16, and 8)\\n- YOLOv2 Struggled with small object detection - solved here'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 36, 'page_label': '37'}, page_content='YOLOv3: An Incremental Improvement\\n- Architecture\\n[ref: https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b]'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 37, 'page_label': '38'}, page_content='YOLOv3:An Incremental Improvement\\nResults\\n- 3x faster but in terms of mAP, loses out to RetinaNet but very much comparable.\\n- PS: about Focal loss, They tried using focal loss, but It dropped their mAP about 2 \\npoints.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 38, 'page_label': '39'}, page_content='YOLOv4: Optimal Speed and Accuracy of Object Detection\\nRefs:\\n- https://arxiv.org/pdf/2004.10934.pdf \\n- https://sh-tsang.medium.com/review-yolov4-optimal-speed-and-accuracy-of-o\\nbject-detection-8198e5b37883 \\nAims and results:\\n- More accurate but just as Fast. \\n- Improves YOLOv3’s AP and FPS by 10% and 12%, respectively.\\n- extended as Scaled-YOLOv4'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 39, 'page_label': '40'}, page_content='YOLOv4: Optimal Speed and Accuracy of Object Detection\\nIntroduces some definitions which are colloquially used in Object Detection:\\n- Bag of freebies\\n- Methods that only change the training strategy or only increase the training cost - Data \\nAugmentation Strategies\\n- Random erase, CutOut, Hide-and-seek, grid-mask, DropOut, DropConnect, DropBlock, \\nMixUp, Using style transfer GAN, label smoothing, IoU losses.\\n- Bag of specials\\n- Methods that only increase the inference cost by a small amount but can significantly improve \\nthe accuracy of object detection - architectural changes\\n- SPP, ASPP, RFB, Spatial Pyramid Matching (SPM), Skip connections'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 40, 'page_label': '41'}, page_content='Architecture:\\nYOLOv4 uses CSPDarknet53 as the backbone, SPP and PANet as the neck, and YOLOv3 as the head.\\n- Cross Stage Partial Network (CSPNet): \\nhttps://sh-tsang.medium.com/review-cspnet-a-new-backbone-that-can-enhance-learning-capability-of-cnn-da7c\\na51524bf \\n- Spatial Pyramid Pooling (SSP): \\nhttps://medium.com/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classificatio\\nn-in-ilsvrc-906da3753679 \\n- Path Aggregation Network (PANet): \\nhttps://becominghuman.ai/reading-panet-path-aggregation-network-1st-place-in-coco-2017-challenge-instance-\\nsegmentation-fe4c985cad1b \\nYOLOv4: Optimal Speed and Accuracy of Object Detection'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 41, 'page_label': '42'}, page_content='Architecture:\\nYOLOv4: Optimal Speed and Accuracy of Object Detection\\n[image ref: https://aiacademy.tw/yolo-v4-intro/]'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 42, 'page_label': '43'}, page_content='YOLOv4: Optimal Speed and Accuracy of Object Detection\\n- Additional Improvements:\\n- Using a Mosaics of images (augmentation technique) to train. Helps with context mixing. \\nReduced need of large mini-batches.\\n- Self Adversarial Training (SAT) - P1: altering the image instead of the weights, P2: detect \\nobject in this image.\\n- Cross mini-Batch Normalization (CmBN) - Collects statistics only between smaller portions of \\na mini-batch\\n[img ref: https://arxiv.org/pdf/2004.10934.pdf ]'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 43, 'page_label': '44'}, page_content='YOLOv4: Optimal Speed and Accuracy of Object Detection\\nSome Bag of Freebies and Bag of Specials ablation results:\\nInfluence of BoF and Mish on the CSPDarknet-53 classifier accuracy'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 44, 'page_label': '45'}, page_content='YOLOv4: Optimal Speed and Accuracy of Object Detection\\nPerformance:\\n- YOLOv4 outperforms EfficientDet, \\nASFF, NAS-FPN, CenterNet, \\nCornerNet, etc.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 45, 'page_label': '46'}, page_content='Scaled YOLOv4/YOLOv4-CSP\\nRefs:\\n- https://arxiv.org/pdf/2011.08036.pdf \\n- https://sh-tsang.medium.com/review-scaled-yolov4-scaling-cross-stage-partial-network-51e3c515b0a7 \\nOverview:\\n- Employs scaling of size, depth, and width of the network layers with Cross Stage Partial Network (CSPNet) \\nblocks.\\n- CSP to every section of YOLOv4 network - Backbone, Neck, and Head.\\n- Essentially faster inference for larger resolution capacities.\\n- A fully CSP-ized model YOLOv4-P5 is designed and can be scaled up to YOLOv4-P6 and YOLOv4-P7.\\nIf interested, please refer to their paper for ablation studies.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 46, 'page_label': '47'}, page_content='Scaled YOLOv4/YOLOv4-CSP\\nResults:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 47, 'page_label': '48'}, page_content='PP-YOLO\\nRefs:\\n- https://arxiv.org/pdf/2007.12099.pdf \\n- https://pyimagesearch.com/2022/04/04/introduction-to-the-yolo-family/ \\nOverview:\\n- PaddlePaddle is a deep learning framework written by Baidu, which has a \\nmassive repository of Computer Vision and Natural Language Processing \\nmodels)\\n- Their YOLOv3 implementations pushed further with larger batches, \\nExponential decay, DropBlock, SSP, CoordConv etc.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 48, 'page_label': '49'}, page_content='PP-YOLO\\nResults:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 49, 'page_label': '50'}, page_content='Refs:\\n- https://github.com/ultralytics/yolov5 - June 2020\\n- Only YOLO without a paper - Still patched by Ultralytics\\nOverview:\\n- Native implementation in PyTorch, int16 training - helped.\\n- Data Augmentation Techniques from YOLOv4\\n- Architecture very close to YOLOv3 - and then improved on eventually\\n- Great documentation on Training custom dataset, and multi-GPU training - Test run on Google Colab \\nPro\\n- Benchmarked to be faster than YOLOv4 for the same mAP scores\\n- Comes in nano, small, medium, large and extra large sizes\\nYOLOv5'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 50, 'page_label': '51'}, page_content='Architecture:\\n- Backbone: A convolutional neural network that aggregates and forms image features at different granularities. (CSP)\\n- Neck: A series of layers to mix and combine image features to pass them forward to prediction. (PANet)\\n- Head: Consumes features from the neck and takes box and class prediction steps.\\nYOLOv5\\n[ ref: https://blog.roboflow.com/yolov5-improvements-and-evaluation/ ]'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 51, 'page_label': '52'}, page_content='YOLOv5\\nPerformance:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 52, 'page_label': '53'}, page_content='YOLOX\\nRefs: \\n- https://arxiv.org/pdf/2107.08430.pdf \\n- https://github.com/Megvii-BaseDetection/YOLOX \\nOverview:\\n- Switch back to detector to an anchor-free manner.\\n- Anchors were domain specific - less generalized\\n- Detection heads were more complicated\\n- Now the predictions were directly distance from top and left, plus height and width of the \\ndetection. - faster and better performance.\\n- Sample close locals as positives - Center Sampling in FCOS\\n- SimOTA (based on Optimal Transport Assignment for Object Detection)\\n-'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 53, 'page_label': '54'}, page_content=\"YOLOX\\nOverview (cont.)\\n- A decoupled head and the leading label assignment strategy SimOTA to \\nachieve state-of-the-art results.\\n- experiments indicate that the coupled detection head may harm the \\nperformance.\\n- Decoupling these heads improves speed of convergence, but decreases AP,\\n- They use 'little decoupled head' - -1.1ms in speed\\n-\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 54, 'page_label': '55'}, page_content='YOLOX\\nArchitecture:\\n- Shows how the \\nends are split into \\ndifferent modules \\nfor classification, \\nRegression and \\nP(obj)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 55, 'page_label': '56'}, page_content='YOLOX\\nPerformance:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 56, 'page_label': '57'}, page_content='YOLOv7: Trainable Bag-of-Freebies Sets New \\nState-of-the-Art for Real-Time Object Detectors\\nRefs:\\n- https://arxiv.org/pdf/2207.02696.pdf \\n- https://sh-tsang.medium.com/review-yolov7-trainable-bag-of-freebies-sets-new-state\\n-of-the-art-for-real-time-object-detectors-b29f33c041a8 \\nArchitecture:\\n- By controlling the shortest longest gradient path, a deeper network can learn and \\nconverge effectively (‘Designing network design strategies’ paper). In this paper, \\nthey propose Extended-ELAN (E-ELAN) based on ELAN.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 57, 'page_label': '58'}, page_content='YOLOv7: Trainable Bag-of-Freebies Sets New \\nState-of-the-Art for Real-Time Object Detectors\\nArchitecture (cont.)\\n- a convolutional layer with residual or concatenation is replaced by \\nre-parameterized convolution, there should be no identity connection.\\n- Auxiliary Heads for different resolution of detection\\n- Deep supervision needs to be trained on the target objectives regardless of \\nthe circumstances of auxiliary head or lead head. - unexplored and hence;\\n- New Label assigner for each of the heads (Lead head guided label assigner \\nand Coarse-to-fine lead head guided label assigner)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 58, 'page_label': '59'}, page_content='YOLOv7: Trainable Bag-of-Freebies Sets New \\nState-of-the-Art for Real-Time Object Detectors\\nPerformance:\\n- highest accuracy 56.8% AP among all known real-time object detectors with 30 FPS \\nor higher on GPU V100.\\n- YOLOv7-E6 object detector (56 FPS V100, 55.9% AP) outperforms both \\ntransformer-based detector SWINL Cascade-Mask R-CNN (9.2 FPS A100, 53.9% \\nAP) by 509% in speed and 2% in accuracy.\\n- YOLOv7-E6 also beats convolutional based detector ConvNeXt-XL Cascade-Mask \\nR-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed and 0.7% AP in accuracy.\\n-'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 59, 'page_label': '60'}, page_content='YOLOv7: Trainable Bag-of-Freebies Sets New \\nState-of-the-Art for Real-Time Object Detectors\\nPerformance(cont.)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 60, 'page_label': '61'}, page_content='YOLOv6: A Single-Stage Object Detection Framework for \\nIndustrial Applications\\nRefs:\\n- https://arxiv.org/pdf/2209.02976.pdf - June 2022\\n- https://github.com/meituan/YOLOv6 \\n- https://learnopencv.com/yolov6-object-detection/ \\nMotives:\\n- It if for use in industrial applications, trained for more epochs (400)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 61, 'page_label': '62'}, page_content='YOLOv6: A Single-Stage Object Detection Framework for \\nIndustrial Applications\\nArchitecture and Labels:\\n- Experiments show SimOTA and TAL (Task Aligned Assignment) are the best \\nlabel assignment strategies for anchor-free object detection models\\n- Revised Reparameterized backbone and neck -  In reparameterization, the \\nnetwork structure changes during training and inference \\n- The entire backbone of the YOLOv6 architecture is called EfficientRep - \\nEfficientRep Torch Implementation on HuggingFace \\n- Medium and Large models, the YOLOv6 architecture uses reparameterized \\nversions of the CSP backbone. They call it the CSPStackRep.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 62, 'page_label': '63'}, page_content='YOLOv6: A Single-Stage Object Detection Framework for \\nIndustrial Applications\\nArchitecture:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 63, 'page_label': '64'}, page_content='YOLOv6: A Single-Stage Object Detection Framework for \\nIndustrial Applications\\nLoss:\\n- VeriFocal Loss (VFL) - as classification loss\\n- Distribution Focal Loss (DFL) - along with SIoU or GIoU as box regression loss\\nPerformance:\\n- YOLOv6 Nano model has achieved an mAP of 35.6% on the COCO dataset. Also, it \\nruns at more than 1200 FPS on an NVIDIA Tesla T4 GPU with a batch size of 32.\\n- YOLOv6-L model gives an mAP of 52.5% on the COCO validation dataset while still \\nbeing able to maintain an FPS of 121.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 64, 'page_label': '65'}, page_content='YOLOv6: A Single-Stage Object Detection Framework for \\nIndustrial Applications\\nPerformance (cont):'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 65, 'page_label': '66'}, page_content=''),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 66, 'page_label': '67'}, page_content='(other) SOTA Architectures \\n● Detection:\\n○ MobileNet\\n○ SqueezeNet\\n○ SIFT\\n● Segmentation\\n○ U-Net\\n○ Fast FCN\\n○ Gated SCNN\\n○ DeepLab\\n○ Loss Functions'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 67, 'page_label': '68'}, page_content='MobileNet\\n● Proposed by Andrew G. Howard in 2017 \\n(https://arxiv.org/abs/1704.04861)\\n● Uses depth-wise separable convolutions \\nto build light weight deep neural \\nnetworks.\\n● Introduces two simple global \\nhyper-parameters that efficiently trade \\noff between latency and accuracy, \\nallowing the model builder to choose the \\nright sized model for their application \\nbased on the constraints of the problem'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 68, 'page_label': '69'}, page_content='MobileNet'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 69, 'page_label': '70'}, page_content='SqueezeNet\\n● Introduced by Forrest N. Iandola et.al. in \\n2016 (https://arxiv.org/abs/1602.07360)\\n● Smaller DNNs require less \\ncommunication across servers during \\ndistributed training.\\n● Smaller DNNs require less bandwidth to \\nexport a new model from the cloud to an \\nautonomous car.\\n● Smaller DNNs are more feasible to \\ndeploy on FPGAs and other hardware \\nwith limited memory\\n● Derivative of AlexNet (and in a way, \\nLeNet)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 70, 'page_label': '71'}, page_content='SqueezeNet'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 71, 'page_label': '72'}, page_content='SIFT\\n● Introduced by D. Lowe et.al. in 2004\\n● Non-learning based approach, \\nScale-Invariant Feature Transform \\nhas 4 main steps:\\n○ Scale-space peak selection: Potential \\nlocation for finding features.\\n○ Keypoint Localization: Accurately locating \\nthe feature keypoints.\\n○ Orientation Assignment: Assigning \\norientation to keypoints.\\n○ Keypoint descriptor: Describing the \\nkeypoints as a high dimensional vector.\\n○ Keypoint Matching\\nImages in this slide from https://medium.com/data-breach/introduction-to-sift-scale-invariant-feature-transform-65d7f3a72d40'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 72, 'page_label': '73'}, page_content='U-Net\\n● Proposed by Olaf Ronneberger et \\nal. in 2015 \\n(https://arxiv.org/abs/1505.04597)\\n● Consists of a contracting path to \\ncapture context and a symmetric \\nexpanding path that enables \\nprecise localization\\n● Key elements:\\n○ Encoder Network\\n○ Skip Connections\\n○ Bridge\\n○ Decoder Network'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 73, 'page_label': '74'}, page_content='Fast FCN\\n● Introduced by Huikai Wu et al. in 2019 (https://arxiv.org/abs/1903.11816)\\n● Proposes a novel joint upsampling module named Joint Pyramid Upsampling (JPU) by formulating \\nthe task of extracting high-resolution feature maps into a joint upsampling problem\\n● Reduces the computation complexity by more than three times without performance loss'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 74, 'page_label': '75'}, page_content='Fast FCN'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 75, 'page_label': '76'}, page_content='Gated-SCNN\\n● Introduced by Towaki Takikawa et al. in \\n2019 in collaboration with NVIDIA \\n(https://arxiv.org/pdf/1907.05740.pdf)\\n● Proposes a new two-stream CNN \\narchitecture for semantic segmentation \\nthat explicitly wires shape information \\nas a separate processing branch'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 76, 'page_label': '77'}, page_content='Gated-SCNN'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 77, 'page_label': '78'}, page_content='DeepLab\\n● Introduced by Liang-Chieh CHen et al. in \\n2014 (https://arxiv.org/abs/1412.7062v4) \\n● First, the input image goes through the \\nnetwork with the use of dilated \\nconvolutions. Then the output from the \\nnetwork is bilinearly interpolated and \\ngoes through the fully connected CRF to \\nfine tune the result to obtain the final \\npredictions.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 78, 'page_label': '79'}, page_content='DeepLab'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 79, 'page_label': '80'}, page_content='Segmentation Loss Functions\\nSemantic segmentation models usually use a simple cross-categorical entropy \\nloss function during training. However, if you are interested in getting the granular \\ninformation of an image, then you have to revert to slightly more advanced loss \\nfunctions.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 80, 'page_label': '81'}, page_content='Segmentation Loss Functions\\n● Focal Loss:\\n● Dice Loss:\\n● Intersection over Union (IoU)-balanced Loss:\\n● Weighted cross-entropy: \\n● Boundary Loss:\\n● Lovasz-Softmax Loss:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 81, 'page_label': '82'}, page_content='Segmentation Loss Functions\\n● TopK loss: \\n○ Ensure that networks concentrate on hard samples during the training process.\\n● Distance penalized CE loss: \\n○ Directs the network to boundary regions that are hard to segment.\\n● Sensitivity-Specificity (SS) loss: \\n○ Computes the weighted sum of the mean squared difference of specificity and sensitivity.\\n● Hausdorff distance(HD) loss: \\n○ Estimates the Hausdorff distance from a convolutional neural network.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'YOLO - Recitation', 'source': './source_data/YOLO.pdf', 'total_pages': 83, 'page': 82, 'page_label': '83'}, page_content='Other References:\\n1. Image Segmentation Using Deep Learning: A Survey by Shervin et. al - \\nhttps://arxiv.org/pdf/2001.05566.pdf \\n2. Rich feature hierarchies for accurate object detection and semantic \\nsegmentation - https://arxiv.org/abs/1311.2524v5\\n3. Fast R-CNN - https://arxiv.org/abs/1504.08083\\n4. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal \\nNetworks - https://arxiv.org/abs/1506.01497\\n5. Mask R-CNN - https://arxiv.org/abs/1703.06870')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e47d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
